{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1272,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 3
      },
      {
       "item_id": 4
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5949,
     "status": "error",
     "timestamp": 1520760604752,
     "user": {
      "displayName": "Usama Riaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "100708119964800276422"
     },
     "user_tz": 480
    },
    "id": "sXz3vGTPs2ks",
    "outputId": "c2511b78-6bc7-4059-dd73-07d8a8428ccf",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 100, 15)           16320     \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 100, 256)          4096      \n",
      "=================================================================\n",
      "Total params: 20,416\n",
      "Trainable params: 20,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      " 113/5524 [..............................] - ETA: 13:13 - loss: 4.4451 - acc: 0.1364"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "import numpy as np\n",
    "import pdb\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, Flatten, TimeDistributed\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint\n",
    "\n",
    "num_chars = 256\n",
    "seq_len = 100\n",
    "batch_size = 32\n",
    "model_save_path = 'serialized_models/shakespeare_gen.h5'\n",
    "\n",
    "def bliteral_to_categorical(b_string):\n",
    "    # Convert byte literal representation\n",
    "    int_rep = [ord(c) for c in b_string]\n",
    "    return keras.utils.to_categorical(int_rep, num_classes=num_chars)\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    random_char = chr(np.random.randint(num_chars))\n",
    "    start_char = bliteral_to_categorical(''.join([random_char for i in range(seq_len)]))\n",
    "    curr_model_input = start_char.reshape(1, *start_char.shape)\n",
    "    for i in range(len(start_char) - 1):\n",
    "        curr_index_pred = model.predict(curr_model_input)[0, i]\n",
    "        model_next_input = np.zeros(num_chars)\n",
    "        model_next_input[np.argmax(curr_index_pred)] = 1\n",
    "        curr_model_input[0, i + 1] = model_next_input\n",
    "    \n",
    "    final_output = model.predict(curr_model_input)[0]\n",
    "    best_pred_chars = np.argmax(final_output, axis=1)\n",
    "    str_out = ''.join([chr(r) for r in best_pred_chars])\n",
    "    print('Generated string: {}'.format(str_out))\n",
    "    \n",
    "def generate_sequences(inputs, targets, seq_len, batch_size):\n",
    "    while True:\n",
    "        seq_starts = np.random.choice(len(inputs) - seq_len, batch_size)\n",
    "        X_batch = [inputs[s:s+seq_len] for s in seq_starts]\n",
    "        Y_batch = [targets[s:s+seq_len] for s in seq_starts]\n",
    "        yield np.array(X_batch), np.array(Y_batch)\n",
    "\n",
    "with open('data/hamlet.txt', 'r') as f:\n",
    "    model = Sequential([\n",
    "        LSTM(15, input_shape=(seq_len, num_chars), return_sequences=True),\n",
    "        TimeDistributed(Dense(num_chars, activation='softmax'))\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary() \n",
    "    \n",
    "    input_text = f.read()\n",
    "    \n",
    "    targets = input_text[1:]\n",
    "    input_text = input_text[:(len(input_text) - 1)]\n",
    "      \n",
    "    oh_input_chars = bliteral_to_categorical(input_text)\n",
    "    oh_targets = bliteral_to_categorical(targets)\n",
    "    steps_per_epoch = oh_input_chars.shape[0] / batch_size\n",
    "    \n",
    "    train_generator = generate_sequences(oh_input_chars, oh_targets, seq_len=seq_len, batch_size=batch_size)\n",
    "    \n",
    "    print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "    checkpointer = ModelCheckpoint(filepath=model_save_path, verbose=1)\n",
    "    model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=10, callbacks=[print_callback, checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Shakespeare Generator.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
